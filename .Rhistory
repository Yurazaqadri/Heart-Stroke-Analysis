tidy(stroke_fit)
stroke_training_baked <- Stroke_main %>%
prep() %>%
bake(new_data = stroke_training)
# View results
stroke_training_baked
stroke_lm_fit <- lr_mod %>%
fit(stroke ~ ., data = stroke_training_baked)
vip(stroke_lm_fit)
vip(stroke_fit)
# here we are replacing the missing value in metric 2 with mean.
# Calculate the mean of metric_2
mean_metric_2 <- mean(stroke_data$metric_2, na.rm = TRUE)
# Replace missing values in metric_2 with the mean
stroke_data$metric_2[is.na(stroke_data$metric_2)] <- mean_metric_2
#here we can see that there are no missing values
skim(stroke_data)
# step-2 Data Splitting
set.seed(321)
# Create a split object
stroke_split <- initial_split(stroke_data, prop = 0.80,
strata = stroke)
# Build training data set
stroke_training <- stroke_split %>%
training()
# Build testing data set
stroke_testing <- stroke_split %>%
testing()
#Displaying Training Data
stroke_training
#Step-3 Feature Engineering
#Creating Recipe step_YeoJohnson: This step applies the Yeo-Johnson transformation to all numeric features to transform them into a more normally distributed form.
#step_dummy: This step creates dummy variables for all nominal features to enable the use of these categorical features in machine learning models.
#step_nzv: This step removes predictors that have no or very low variance across the samples. This is done to remove features that are not informative or that might cause overfitting.
Stroke_main<-recipe(stroke~ age,	hypertension,	heart_disease,	occupation,	residence	,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,smoking_status, stroke_training  ,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
#
Stroke_main %>%
prep() %>%
bake(new_data = stroke_training)
lr_mod<-logistic_reg()%>%
set_engine('glm')
stroke_workflow<-workflow()%>%
add_model(lr_mod)%>%
add_recipe(Stroke_main)
stroke_fit <- stroke_workflow %>%
fit(data=stroke_training)
#Step-3 Feature Engineering
#Creating Recipe
#Creating Recipe step_YeoJohnson: This step applies the Yeo-Johnson transformation to all numeric features to transform them into a more normally distributed form.
#step_dummy: This step creates dummy variables for all nominal features to enable the use of these categorical features in machine learning models.
#step_nzv: This step removes predictors that have no or very low variance across the samples. This is done to remove features that are not informative or that might cause overfitting.
stroke_training$stroke <- as.factor(stroke_training$stroke)
stroke_testing$stroke <- as.factor(stroke_testing$stroke)
Stroke_main<-recipe(stroke~ age,	hypertension,	heart_disease,	occupation,	residence	,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,smoking_status, stroke_training  ,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
Stroke_main %>%
prep() %>%
bake(new_data = stroke_training)
lr_mod<-logistic_reg()%>%
set_engine('glm')
stroke_workflow<-workflow()%>%
add_model(lr_mod)%>%
add_recipe(Stroke_main)
stroke_fit <- stroke_workflow %>%
fit(data=stroke_training)
# Here we comparing our predicted values with testing dataset.
# Here we are testing our model with test data .
# Here We train our model with test data set the 20% of actual data set.
predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions<-predict(stroke_fit, stroke_testing, type ="prob")%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions
tidy(stroke_fit)
stroke_predictions<-predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions%>%table()
#install.packages("caret")
library(caret)
predictions = predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
predictions%>%conf_mat(truth=stroke, .pred_class)
caret::confusionMatrix(predictions$.pred_class,predictions$stroke)
#Step-6 Evaluating model
stroke_predictions %>%
roc_curve(
truth = stroke,
.pred_0,
event_level = "first"
)%>%
autoplot()
#Step-6 Evaluating model
stroke_predictions %>%
roc_curve(
truth = stroke,
.pred_1,
event_level = "first"
)%>%
autoplot()
stroke_predictions <-predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions%>%table()
stroke_predictions
stroke_predictions1 <-predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions1%>%table()
stroke_predictions<-predict(stroke_fit, stroke_testing, type ="prob")%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions
tidy(stroke_fit)
stroke_predictions1 <-predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions1%>%table()
#install.packages("caret")
library(caret)
predictions = predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
predictions%>%conf_mat(truth=stroke, .pred_class)
caret::confusionMatrix(predictions$.pred_class,predictions$stroke)
#Step-6 Evaluating model
stroke_predictions %>%
roc_curve(
truth = stroke,
.pred_0,
event_level = "first"
)%>%
autoplot()
stroke_predictions %>%
roc_auc(
truth = stroke,
.pred_0,
event_level = "first")
#Step-3 Feature Engineering
#Creating Recipe
#Creating Recipe step_YeoJohnson: This step applies the Yeo-Johnson transformation to all numeric features to transform them into a more normally distributed form.
#step_dummy: This step creates dummy variables for all nominal features to enable the use of these categorical features in machine learning models.
#step_nzv: This step removes predictors that have no or very low variance across the samples. This is done to remove features that are not informative or that might cause overfitting.
stroke_training$stroke <- as.factor(stroke_training$stroke)
#stroke_testing$stroke <- as.factor(stroke_testing$stroke)
Stroke_main<-recipe(stroke~ age,	hypertension,	heart_disease,	occupation,	residence	,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,smoking_status, stroke_training  ,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Here we are preparing and baking the recipe and setting the model and creating workflow and fitting the model.
Stroke_main %>%
prep() %>%
bake(new_data = stroke_training)
lr_mod<-logistic_reg()%>%
set_engine('glm')
stroke_workflow<-workflow()%>%
add_model(lr_mod)%>%
add_recipe(Stroke_main)
stroke_fit <- stroke_workflow %>%
fit(data=stroke_training)
# Here we comparing our predicted values with testing dataset.
# Here we are testing our model with test data .
# Here We train our model with test data set the 20% of actual data set.
predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
#Here we are predicting person who have stroke which is 1 and person who doesn't have stroke which is 0
stroke_predictions<-predict(stroke_fit, stroke_testing, type ="prob")%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions
tidy(stroke_fit)
# Confussion Matrix is poltted below:
# As you can see there are no type 2 errors which are false negative and there are 156 false positive errors which means
stroke_predictions1 <-predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions1%>%table()
#Here we are checking the accuracy which is
#install.packages("caret")
library(caret)
predictions = predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
predictions%>%conf_mat(truth=stroke, .pred_class)
caret::confusionMatrix(predictions$.pred_class,predictions$stroke)
#Step-6 Evaluating model
stroke_predictions %>%
roc_curve(
truth = stroke,
.pred_0,
event_level = "first"
)%>%
autoplot()
newdata <- tibble(
num_char = c(2, 15, 40),
color = c("#A7D5E8", "#e9d968", "#8fada7"),
shape = c(22, 24, 23)
)
# Predict on new data
y_hat <- predict(spam_fit, newdata, type = "raw")
p_hat <- exp(y_hat) / (1 + exp(y_hat))
# Add predictions to new data
newdata <- newdata %>%
bind_cols(
y_hat = y_hat,
p_hat = p_hat
)
# Create augmented data
spam_aug <- augment(spam_fit$fit) %>%
mutate(prob = exp(.fitted) / (1 + exp(.fitted)))
# Create plot
ggplot(spam_aug, aes(x = num_char)) +
geom_point(aes(y = as.numeric(spam) - 1, color = spam), alpha = 0.3) +
scale_color_manual(values = c("#E69F00", "#56B4E9")) + # Change color scheme
scale_y_continuous(breaks = c(0, 1)) +
guides(color = "none") +
geom_line(aes(y = prob)) +
geom_point(data = newdata, aes(x = num_char, y = p_hat),
fill = newdata$color, shape = newdata$shape,
stroke = 1, size = 6) +
labs(
x = "Number of characters (in thousands)",
y = "Spam",
title = "Spam vs. number of characters"
)
library(yardstick)
# Make predictions on testing data
stroke_pred <- predict(stroke_fit, stroke_testing)
# Evaluate performance using AUC
stroke_auc <- roc_auc(truth = stroke_testing$stroke,
estimate = stroke_pred$stroke)
library(yardstick)
# Make predictions on testing data
stroke_pred <- predict(stroke_fit, stroke_testing)
# Evaluate performance using AUC
stroke_auc <- roc_auc(truth = stroke$stroke,
estimate = stroke_pred$stroke)
library(yardstick)
# Make predictions on testing data
stroke_pred <- predict(stroke_fit, stroke_testing)
# Evaluate performance using AUC
stroke_auc <- roc_auc(truth = stroke_testing$stroke,
estimate = stroke_pred$stroke)
#Plotting the area in number which is 85%.
stroke_predictions %>%
roc_auc(
truth = stroke,
.pred_0,
event_level = "first")
#Here we are checking the accuracy which is 98% accurate model
#install.packages("caret")
library(caret)
predictions = predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
predictions%>%conf_mat(truth=stroke, .pred_class)
caret::confusionMatrix(predictions$.pred_class,predictions$stroke)
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 10, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main)
# Fit and evaluate the model using cross-validation
lr_results <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 10, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ age,	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	, stroke_training  ,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
lr_results <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
#Step-3 Feature Engineering
#Creating Recipe
#Creating Recipe step_YeoJohnson: This step applies the Yeo-Johnson transformation to all numeric features to transform them into a more normally distributed form.
#step_dummy: This step creates dummy variables for all nominal features to enable the use of these categorical features in machine learning models.
#step_nzv: This step removes predictors that have no or very low variance across the samples. This is done to remove features that are not informative or that might cause overfitting.
stroke_training$stroke <- as.factor(stroke_training$stroke)
stroke_testing$stroke <- as.factor(stroke_testing$stroke)
Stroke_main<-recipe(stroke~ age,	hypertension,	heart_disease,	occupation,	residence	,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,smoking_status, stroke_training  ,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Here we are preparing and baking the recipe and setting the model and creating workflow and fitting the model.
Stroke_main %>%
prep() %>%
bake(new_data = stroke_testing)
lr_mod<-logistic_reg()%>%
set_engine('glm')
stroke_workflow<-workflow()%>%
add_model(lr_mod)%>%
add_recipe(Stroke_main)
stroke_fit <- stroke_workflow %>%
fit(data=stroke_testing)
# Here we comparing our predicted values with testing dataset.
# Here we are testing our model with test data .
# Here We train our model with test data set the 20% of actual data set.
predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
#Here we are predicting person who have stroke which is 1 and person who doesn't have stroke which is 0
stroke_predictions<-predict(stroke_fit, stroke_testing, type ="prob")%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions
tidy(stroke_fit)
# Confussion Matrix is poltted below:
# As you can see there are no type 2 errors which are false negative and there are 156 false positive errors which means
stroke_predictions1 <-predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
stroke_predictions1%>%table()
#Here we are checking the accuracy which is 98% accurate model
#install.packages("caret")
library(caret)
predictions = predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
predictions%>%conf_mat(truth=stroke, .pred_class)
caret::confusionMatrix(predictions$.pred_class,predictions$stroke)
#Step-6 Evaluating model
# As much as area is there under the line the model is more accurate.
stroke_predictions %>%
roc_curve(
truth = stroke,
.pred_0,
event_level = "first"
)%>%
autoplot()
#Plotting the area in number which is 85%.
stroke_predictions %>%
roc_auc(
truth = stroke,
.pred_0,
event_level = "first")
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 5, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ age,	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	, stroke_training  ,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
lr_results <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 5, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ age,	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
lr_results <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 5, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ 	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
lr_results <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 5, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ 	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
lr_results <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
lr_results
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 5, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ 	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
stroke_fit2 <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
stroke_fit2
stroke_predictions1 <-predict(stroke_fit2, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 5, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ 	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
stroke_fit2 <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
stroke_fit2
stroke_predictions2 <-predict(stroke_fit2, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 10, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ 	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
stroke_fit2 <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
stroke_fit2
stroke_predictions2 <-predict(stroke_fit2, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
# Here we comparing our predicted values with testing dataset.
# Here we are testing our model with test data .
# Here We train our model with test data set the 20% of actual data set.
predict(stroke_fit, stroke_testing)%>%
bind_cols(stroke_testing%>%select(stroke))
# Define the cross-validation scheme
cv <- vfold_cv(stroke_training, v = 10, strata = stroke)
# Define the model specification
lr_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
#create New Recipe
Stroke_main2<-recipe(stroke~ 	hypertension,	heart_disease,metric_1	,metric_2,	metric_3,	metric_4	,metric_5	,data=stroke_training, family='binomial')%>%
step_YeoJohnson(all_numeric(), - all_outcomes()) %>% ##
step_dummy(all_nominal(), -all_outcomes()) %>%
step_nzv(all_predictors())
# Define the workflow
lr_wf <- workflow() %>%
add_model(lr_spec) %>%
add_recipe(Stroke_main2)
# Fit and evaluate the model using cross-validation
stroke_fit2 <- lr_wf %>%
fit_resamples(resamples = cv) %>%
collect_metrics()
stroke_fit2
